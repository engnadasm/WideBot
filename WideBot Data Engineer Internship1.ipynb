{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"WideBot Data Engineer Internship1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMVnGqvglszzz4LvbZB7HeG"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jySgEAITxNcW","colab_type":"text"},"source":["# Task 1 - Getting to Philosophy\n","**Write a Python script to check the \"Getting to Philosophy\" law.\n","https://en.wikipedia.org/wiki/Wikipedia:Getting_to_Philosophy\n","Clicking on the first link in the main body of a Wikipedia article and repeating the process\n","for subsequent articles would usually lead to the article Philosophy.\n","The program should receive a Wikipedia link as an input, go to another normal link and\n","repeat this process until either Philosophy page is reached, or we are in an article without\n","any outgoing Wikilinks, or stuck in a loop.\n","A \"normal link\" is a link from the main page article, not in a box, is blue (red is for\n","non-existing articles), not in parentheses, not italic and not a footnote. You don't have to\n","check style tables or other fancy things, it is enough that the script works with the current\n","Wikipedia style (for example you can use 'class' attribute in Wikipedia tags). For easy\n","validation, please print all visited links to the standard output.\n","Use a 0.5 second timeout between queries to avoid heavy load on Wikipedia (sleep function\n","from time module).\n","You can use https://en.wikipedia.org/wiki/Special:Random to check this hypothesis at\n","home.**\n"]},{"cell_type":"markdown","metadata":{"id":"EDEZLz-ZyG2a","colab_type":"text"},"source":["\n","\n","---\n","\n","\n","\n","---\n","\n","\n","**First, Install dependencies like: BeautifulSoup, urllib, time, sys & requests where :**\n","\n","*   Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML.\n","\n","*   Urllib module is the URL handling module for python. It is used to fetch URLs.\n","   * urllib.parse for parsing URLs\n","\n","*   time to handle time-related tasks.\n","*   requests module allows you to send HTTP requests using Python.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"DSyzvwKjyvhe","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596741251533,"user_tz":-120,"elapsed":1536,"user":{"displayName":"Nada Salama","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM0li2jooVo_xTUDCbOmtdgW0OLQIzoLTaJtpS=s64","userId":"12431211946354955440"}}},"source":["from bs4 import BeautifulSoup\n","import urllib\n","import time\n","import sys\n","import requests\n"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"rM4xF8Xey2KG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596741252430,"user_tz":-120,"elapsed":2339,"user":{"displayName":"Nada Salama","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM0li2jooVo_xTUDCbOmtdgW0OLQIzoLTaJtpS=s64","userId":"12431211946354955440"}}},"source":["start_url = \"https://en.wikipedia.org/wiki/Special:Random\"\n","target_url = \"https://en.wikipedia.org/wiki/Philosophy\"\n","\n","# to store urls of the visited article \n","visited_urls = [start_url]"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"tPzlujhgza51","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596741252435,"user_tz":-120,"elapsed":2302,"user":{"displayName":"Nada Salama","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM0li2jooVo_xTUDCbOmtdgW0OLQIzoLTaJtpS=s64","userId":"12431211946354955440"}}},"source":["def find_first_link(url):\n","    response = requests.get(url)\n","    html = response.text\n","    soup = BeautifulSoup(html, \"html.parser\")\n","    link = None\n","   \n","    for anchor in soup.find(id=\"mw-content-text\").find(class_=\"mw-parser-output\").find_all(\"p\", recursive=False):\n","      if anchor.find(\"a\", recursive=False):\n","        link = anchor.find(\"a\", recursive=False).get('href')\n","        break\n","\n","    if not link:\n","        return\n","\n","    # Build a full url \n","    first_link = urllib.parse.urljoin(\n","        'https://en.wikipedia.org/', link)\n","\n","    return first_link"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mddi_2DpzcAk","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596741274691,"user_tz":-120,"elapsed":2430,"user":{"displayName":"Nada Salama","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM0li2jooVo_xTUDCbOmtdgW0OLQIzoLTaJtpS=s64","userId":"12431211946354955440"}}},"source":["def continue_scraping(visited_urls, target_url):\n","    max_steps = 100\n","    # When reaches to philosphy\n","    if visited_urls[-1] == target_url:\n","        print(\"Target ('Philosphy') article reached!\")\n","        return False\n","    # max iterations \n","    elif len(visited_urls) > max_steps:\n","        print(\"Maximum (100) searches reached, interrupted.\")\n","        return False\n","    elif visited_urls[-1] in visited_urls[:-1]:\n","        print(\"We are in a Loop , interrupted.\")\n","        return False\n","    else:\n","        return True"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z3CqUDnrzgxP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1596741278672,"user_tz":-120,"elapsed":5653,"user":{"displayName":"Nada Salama","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM0li2jooVo_xTUDCbOmtdgW0OLQIzoLTaJtpS=s64","userId":"12431211946354955440"}},"outputId":"3355f541-b52a-4363-ef27-3d386b739571"},"source":["while continue_scraping(visited_urls, target_url):\n","    #print first link\n","    print(visited_urls[-1])\n","\n","    first_link = find_first_link(visited_urls[-1])\n","    # when arrive at an article with no links\n","    if not first_link:\n","        print(\"Arrived at an article with no links, search aborted.\")\n","        break\n","        \n","    visited_urls.append(first_link)\n","    # Use a 0.5 second timeout between queries to avoid heavy load on Wikipedia\n","    time.sleep(0.5) \n","visited_urls=[start_url]"],"execution_count":37,"outputs":[{"output_type":"stream","text":["https://en.wikipedia.org/wiki/Special:Random\n","https://en.wikipedia.org/wiki/Persian_language\n","https://en.wikipedia.org/wiki/Exonym_and_endonym\n","https://en.wikipedia.org/wiki/Greek_language\n","Arrived at an article with no links, search aborted.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fRdmNKxizk-z","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596741252446,"user_tz":-120,"elapsed":2180,"user":{"displayName":"Nada Salama","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM0li2jooVo_xTUDCbOmtdgW0OLQIzoLTaJtpS=s64","userId":"12431211946354955440"}}},"source":[""],"execution_count":null,"outputs":[]}]}